\documentclass[12pt, oneside]{article}
\usepackage[T2A]{fontenc}

\usepackage[
  letterpaper,
  left=2.5cm,
  right=2.5cm,
  top=2.5cm,
  bottom=1.5cm
]{geometry}

\usepackage[unicode]{hyperref}
\hypersetup{
    colorlinks=true,
    allcolors=black
}

\usepackage{fancyhdr}
\fancyhf{}
\pagestyle{fancy}
\fancyhead[L]{MIPT CDS}
\fancyhead[C]{Reinforcement Learning}
\fancyhead[R]{Spring 2022}

\author{Due March 11}
\title{Assignment 1}
\date{}


\begin{document}
\maketitle
\thispagestyle{fancy}

\section{Theoretical assignment: TD($\lambda$)}

Данное теоретическое задание посвящено алгоритму TD($\lambda$).

\textbf{Задание TD.1} Доказать, что если вычислять обновления весов на каждом шаге (без их применения), то суммарное обновление онлайнового и оффлайного варианта одинаково.

\textit{Под оффлайновым обновлением подразумевается оффлайновый $\lambda$-доходный алгоритм. Под онлайновым обновлением подразумевается алгоритм TD($\lambda$). Подсказки можно найти в заданиях в конце главы TD($\lambda$) книги Саттона и Барто.}

\textbf{Задание TD.2} Доказать равнозначность оффлайного $\lambda$-доходного алгоритма и истинно онлайнового алгоритма TD($\lambda$).

\textit{См. соответствующую главу Истинно онлайновый TD($\lambda$) книги Саттона и Барто и статью \href{https://www.jmlr.org/papers/volume17/15-599/15-599.pdf}{van Seijen et al., 2016}}.

\section{Practical assignment: Imitation learning}

Цель задания - реализовать и поэкспериментировать с алгоритмами имитационного обучения. По имеющимся демонстрациям предлагается обучить стратегии с помощью клонирования поведения и алгоритма DAgger (см \href{https://youtu.be/HUzyjOsd2PA?list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc}{лекция по имитации поведения}, \href{http://proceedings.mlr.press/v15/ross11a/ross11a.pdf}{статья по DAgger}), а затем сравнить их качество в Gym-средах на базе Mujoco. Данное задание основано на первом задании курса по \href{http://rail.eecs.berkeley.edu/deeprlcourse/}{Deep RL Университета Беркли}.

Код задания расположен по адресу
\begin{center}
    \href{https://github.com/pkuderov/mipt-rl-hw-2022}{https://github.com/pkuderov/mipt-rl-hw-2022}
\end{center}

Есть возможность выполнять задание как локально, так и в Google Colab. Подробности, в том числе по установке зависимостей, вы найдете в README к первому дз.

\subsection{Behavioral Cloning}

Требуется заполнить пропуски в реализации клонирования поведения. По адресу \verb|expert_data| содержатся \textit{pickled} данные экспертных стратегий. Рекомендуется начать ознакомление с входной точки запуска \verb|scripts/run_hw1.py| и затем продвигаться вглубь к

\begin{itemize}
    \item \verb|infrastructure/rl_trainer.py|,
    \item \verb|agents/bc_agent.py|,
    \item \verb|policies/MLP_policy.py|
    \item и далее, заполняя пропуски, отмеченные \verb|TODO|.
\end{itemize}

\textbf{Задание BC.1} Получите результаты вашей реализации в двух средах: Ant и любой другой. В Ant качество должно быть не хуже $25\%$ от качества эксперта. Пример запуска найдете в README к дз. Требуются следующие данные: среднее значение и стандартное отклонение отдачи по набору тестовых эпизодов. Эти данные логируются как \verb|Eval_AverageReturn| и \verb|Eval_StdReturn| соответственно.

\textit{Обратите внимание, чтобы собрать данные по нескольким эпизодам, потребуется при запуске указать параметр суммарного кол-ва шагов \texttt{eval\_batch\_size} в несколько раз больше параметра максимальной длины эпизода \texttt{ep\_len}. Собственно, их отношение и задает минимальное число эпизодов, которые войдут в выборку.}

\textit{Выключение генерации видео при логировании достигается с использованием аргумента запуска} \verb|--video_log_freq -1|. \textit{Удалите его при необходимости, но помните о возможном существенном замедлении выполнения.}

\textbf{Задание BC.2} Поэкспериментируйте с разными наборами гиперпараметров (например, число шагов обучения, объем предоставленных экспертных данных и тп). Для одного фиксированного гиперпараметра постройте график изменения качества работы агента [на одной из сред из первого задания] и поясните ваш выбор этого параметра.

\subsection{DAgger}

Заполнив все пропуски \verb|TODO|, вы также сможете запустить DAgger (см. аргументы запуска в README).

\textbf{Задание DA.1} Проведите запуски алгоритма на выбранных в задании BC.1 средах. Постройте график обучения DAgger'а---число итераций алгоритма vs. средняя отдача за эпизод (с указанием стандартного отклонения). Добавь в этот график горизонтальными линиями результаты эксперта и клонирования поведения. Укажи использованные гиперпараметры.

\section{Формат сдачи}

Сдача предполагается в виде пулл реквестов в репозитории по \href{https://github.com/pkuderov/mipt-rl-hw-2022}{ссылке в начале практического задания}.

Ожидается, что пулл реквест будет содержать непосредственно код заполненных вами недостающих частей выданного решения. Также в сообщении к пулл реквесту вы добавляете результаты/решение по каждому из пунктов задания. Маркдаун гитхаба позволяет и вставку картинок, и оформление табличек. Опционально, вы можете оформить ответ в виде отдельного doc или pdf-файла и добавить их в коммит, а в сообщении сослаться на этот файл.

\end{document}
